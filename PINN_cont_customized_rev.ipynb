{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scipy.io as sp\n",
    "from functools import partial\n",
    "from pyDOE import lhs\n",
    "from torch.autograd import Variable\n",
    "\n",
    "iter = 0\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"\n",
    "    Seeding the random variables for reproducibility\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def derivative(dy: torch.Tensor, x: torch.Tensor, order: int = 1) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This function calculates the derivative of the model at x_f\n",
    "    \"\"\"\n",
    "    for i in range(order):\n",
    "        dy = torch.autograd.grad(\n",
    "            dy, x, grad_outputs=torch.ones_like(dy), create_graph=True, retain_graph=True)[0]\n",
    "    return dy\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$u_t-u_{xx}=0,\\ x\\in[0,1], t\\in [0,T],$$\n",
    "$$u(0,x)=\\sin (2\\pi x),$$\n",
    "$$u(t,0)=0,$$\n",
    "$$u_x(t,1)=2\\pi e^{-t}.$$\n",
    "We may define $f$ as $u_t-u_{xx}$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Wave(nn.Module):\n",
    "    \"\"\"\n",
    "    Define the SchrodingerNN,\n",
    "    it consists of 5 hidden layers\n",
    "    \"\"\"\n",
    "    def __init__(self, ):\n",
    "        # Input layer\n",
    "        super(Wave, self).__init__()\n",
    "        self.linear_in = nn.Linear(2, 100)\n",
    "        # Output layer\n",
    "        self.linear_out = nn.Linear(100, 1)\n",
    "        # Hidden Layers\n",
    "        self.layers = nn.ModuleList(\n",
    "            [nn.Linear(100, 100) for i in range(5)]\n",
    "        )\n",
    "        # Activation function\n",
    "        self.act = nn.Tanh() # How about LeakyReLU? Or even Swish?\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear_in(x)\n",
    "        for layer in self.layers:\n",
    "            x = self.act(layer(x))\n",
    "        x = self.linear_out(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def f(model, x_f, t_f):\n",
    "    \"\"\"\n",
    "    This function evaluates the PDE at collocation points.\n",
    "    \"\"\"\n",
    "    u = model(torch.stack((x_f, t_f), axis = 1)) # Concatenates a seq of tensors along a new dimension\n",
    "    u_t = derivative(u, t_f, order=1)\n",
    "    u_xx = derivative(u, x_f, order=2)\n",
    "    return u_t - u_xx\n",
    "\n",
    "\n",
    "def mse_f(model, x_f, t_f):\n",
    "    \"\"\"\n",
    "    This function calculates the MSE for the PDE.\n",
    "    \"\"\"\n",
    "    f_u = f(model, x_f, t_f)\n",
    "    return (f_u ** 2).mean()\n",
    "\n",
    "\n",
    "def mse_0(model, x_0, u_0):\n",
    "    \"\"\"\n",
    "    This function calculates the MSE for the initial condition.\n",
    "    u_0 is the real values\n",
    "    \"\"\"\n",
    "    t_0 = torch.zeros_like(x_0) # creating a t_0 with zero values\n",
    "    f = model(torch.stack((x_0, t_0), axis=1))\n",
    "    # extracting the u and v values from the model output\n",
    "    return ((f - u_0) ** 2).mean()\n",
    "\n",
    "def mse_b(model, t_b):\n",
    "    \"\"\"\n",
    "    This function calculates the MSE for the boundary condition.\n",
    "    \"\"\"\n",
    "    x_b_upper = torch.zeros_like(t_b)\n",
    "    x_b_upper.requires_grad = True\n",
    "    u_b_upper = model(torch.stack((x_b_upper, t_b),axis = 1))\n",
    "    mse_dirichlet = (u_b_upper**2).mean()\n",
    "\n",
    "    x_b_lower = 2*np.pi*torch.exp(-t_b)\n",
    "    x_b_lower.requires_grad = True\n",
    "    u_b_lower = model(torch.stack((x_b_lower, t_b),axis = 1))\n",
    "    u_x_b_lower = derivative(u_b_lower, x_b_lower, 1)\n",
    "    mse_neumann = ((u_b_lower - u_x_b_lower)**2).mean()\n",
    "\n",
    "    return mse_dirichlet + mse_neumann\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    \"\"\"\n",
    "    This function initializes the weights of the model by the normal Xavier initialization method.\n",
    "    \"\"\"\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "\n",
    "def closure(model, optimizer, x_f, t_f, x_0, u_0, t):\n",
    "    \"\"\"\n",
    "    The closure function to use L-BFGS optimization method.\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    # evaluating the MSE for the PDE\n",
    "    loss = mse_f(model, x_f, t_f) + mse_0(model, x_0, u_0) + mse_b(model, t)\n",
    "    loss.backward(retain_graph=True)\n",
    "    global iter\n",
    "    iter += 1\n",
    "    if iter % 50 == 0:\n",
    "        print(f\" Iteration: {iter}  loss: {loss.item()}\")\n",
    "    #if iter % 100 == 0:\n",
    "    #    torch.save(model.state_dict(), f'Schrodingers_Equation/models/model_LBFGS_{iter}.pt')\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train(model, x_f, t_f, x_0, u_0, t):\n",
    "    # Initialize the optimizer\n",
    "    optimizer = torch.optim.LBFGS(model.parameters(),\n",
    "                                  lr=1,\n",
    "                                  max_iter=50000,\n",
    "                                  max_eval=50000,\n",
    "                                  history_size=50,\n",
    "                                  tolerance_grad=1e-05,\n",
    "                                  tolerance_change=0.5 * np.finfo(float).eps,\n",
    "                                  line_search_fn=\"strong_wolfe\")\n",
    "\n",
    "    # the optimizer.step requires the closure function to be a callable function without inputs\n",
    "    # therefore we need to define a partial function and pass it to the optimizer\n",
    "    closure_fn = partial(closure, model, optimizer, x_f, t_f, x_0, u_0, t)\n",
    "    optimizer.step(closure_fn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "x_ic = np.random.uniform(low=0, high=1.0, size=100)\n",
    "t_ic = np.zeros_like(x_ic)\n",
    "\n",
    "x_bc = np.random.uniform(low=0, high=1.0, size=100)\n",
    "t_bc = np.random.uniform(low=0, high=1.0, size=100)\n",
    "\n",
    "u_ic = np.sin(2*np.pi*x_ic)\n",
    "u_bc = np.zeros_like(u_ic)\n",
    "\n",
    "train_idx = np.random.randint(low=0, high=199, size=(100))\n",
    "x_train = np.append(x_ic, x_bc)[train_idx]\n",
    "t_train = np.append(t_ic, t_bc)[train_idx]\n",
    "u_train = np.append(u_ic, u_bc)[train_idx]\n",
    "\n",
    "train_idx = np.random.randint(low=0, high=99, size=(100))\n",
    "t_x_train = np.random.uniform(low=0.0, high=1.0, size=(100))\n",
    "t_x_train = t_x_train[train_idx]\n",
    "x_x_train = np.zeros_like(t_x_train)+1\n",
    "u_x_train = 2*np.pi*np.exp(-t_x_train)\n",
    "u_x_train = u_x_train[train_idx]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "you can only change requires_grad flags of leaf variables.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [19], line 27\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# Training the model\u001B[39;00m\n\u001B[1;32m     26\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m---> 27\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_f\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt_f\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_ic\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mu_ic\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt_x_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn [11], line 87\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, x_f, t_f, x_0, u_0, t)\u001B[0m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;66;03m# the optimizer.step requires the closure function to be a callable function without inputs\u001B[39;00m\n\u001B[1;32m     85\u001B[0m \u001B[38;5;66;03m# therefore we need to define a partial function and pass it to the optimizer\u001B[39;00m\n\u001B[1;32m     86\u001B[0m closure_fn \u001B[38;5;241m=\u001B[39m partial(closure, model, optimizer, x_f, t_f, x_0, u_0, t)\n\u001B[0;32m---> 87\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclosure_fn\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/torch39/lib/python3.9/site-packages/torch/optim/optimizer.py:140\u001B[0m, in \u001B[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    138\u001B[0m profile_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimizer.step#\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.step\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(profile_name):\n\u001B[0;32m--> 140\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m     obj\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m~/miniconda3/envs/torch39/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001B[0m, in \u001B[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclone():\n\u001B[0;32m---> 27\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/torch39/lib/python3.9/site-packages/torch/optim/lbfgs.py:312\u001B[0m, in \u001B[0;36mLBFGS.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    309\u001B[0m state\u001B[38;5;241m.\u001B[39msetdefault(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_iter\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m    311\u001B[0m \u001B[38;5;66;03m# evaluate initial f(x) and df/dx\u001B[39;00m\n\u001B[0;32m--> 312\u001B[0m orig_loss \u001B[38;5;241m=\u001B[39m \u001B[43mclosure\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    313\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(orig_loss)\n\u001B[1;32m    314\u001B[0m current_evals \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/torch39/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001B[0m, in \u001B[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclone():\n\u001B[0;32m---> 27\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn [11], line 62\u001B[0m, in \u001B[0;36mclosure\u001B[0;34m(model, optimizer, x_f, t_f, x_0, u_0, t)\u001B[0m\n\u001B[1;32m     60\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     61\u001B[0m \u001B[38;5;66;03m# evaluating the MSE for the PDE\u001B[39;00m\n\u001B[0;32m---> 62\u001B[0m loss \u001B[38;5;241m=\u001B[39m mse_f(model, x_f, t_f) \u001B[38;5;241m+\u001B[39m mse_0(model, x_0, u_0) \u001B[38;5;241m+\u001B[39m \u001B[43mmse_b\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     63\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward(retain_graph\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mglobal\u001B[39;00m \u001B[38;5;28miter\u001B[39m\n",
      "Cell \u001B[0;32mIn [11], line 39\u001B[0m, in \u001B[0;36mmse_b\u001B[0;34m(model, t_b)\u001B[0m\n\u001B[1;32m     36\u001B[0m mse_dirichlet \u001B[38;5;241m=\u001B[39m (u_b_upper\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mmean()\n\u001B[1;32m     38\u001B[0m x_b_lower \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m\u001B[38;5;241m*\u001B[39mnp\u001B[38;5;241m.\u001B[39mpi\u001B[38;5;241m*\u001B[39mtorch\u001B[38;5;241m.\u001B[39mexp(\u001B[38;5;241m-\u001B[39mt_b)\n\u001B[0;32m---> 39\u001B[0m x_b_lower\u001B[38;5;241m.\u001B[39mrequires_grad \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     40\u001B[0m u_b_lower \u001B[38;5;241m=\u001B[39m model(torch\u001B[38;5;241m.\u001B[39mstack((x_b_lower, t_b),axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m     41\u001B[0m u_x_b_lower \u001B[38;5;241m=\u001B[39m derivative(u_b_lower, x_b_lower, \u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: you can only change requires_grad flags of leaf variables."
     ]
    }
   ],
   "source": [
    "# Upper and lower bounds of the spatial and temporal domains\n",
    "lb = np.array([0.0, 0.0])\n",
    "ub = np.array([1.0, 1.0])\n",
    "# Number of initial, boundary and collocation points\n",
    "N_f = 20_000  # collocation points\n",
    "\n",
    "# Loading the training points\n",
    "\n",
    "x_ic = Variable(torch.from_numpy(x_ic.astype(np.float32)),requires_grad=True).to(device)\n",
    "u_ic = Variable(torch.from_numpy(u_ic.astype(np.float32)),requires_grad=True).to(device)\n",
    "t_x_train = Variable(torch.from_numpy(t_x_train.astype(np.float32)),requires_grad=True).to(device)\n",
    "# collocation data points using latin hypercube sampling method\n",
    "c_f = lb + (ub - lb) * lhs(2, N_f)\n",
    "x_f = torch.from_numpy(c_f[:, 0].astype(np.float32)).to(device)\n",
    "x_f.requires_grad = True\n",
    "t_f = torch.from_numpy(c_f[:, 1].astype(np.float32)).to(device)\n",
    "t_f.requires_grad = True\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model = Wave().to(device)\n",
    "# Apply the initialization function to the model weights\n",
    "model.apply(init_weights)\n",
    "\n",
    "# Training the model\n",
    "model.train()\n",
    "train(model, x_f, t_f, x_ic, u_ic, t_x_train)\n",
    "# torch.save(model.state_dict(), 'Schrodingers_Equation/models/model_LBFGS.pt')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
